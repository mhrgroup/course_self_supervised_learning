{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ssl_section02_lecture04.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhrgroup/course_self_supervised_learning/blob/main/Section%2002%3A%20Supervised%20Models/ssl_section02_lecture04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 04: Transfer Learning & Fine-Tuning**\n",
        "\n",
        "By the end of this lecture, you will be able to:\n",
        "\n",
        "1. Implement supervised learning with transfer learning and fine-tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "Wt9jAeTSiWju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4.1. Supervised Learning with Transfer Learning and Fine-Tuning**\n",
        "---\n",
        "\n",
        "* One can develop a supervised model using an already trained model.\n",
        "\n",
        "* An already trained model, referred to as the base model, is usually borrowed entirely or partially and modified to learn the current problem quickly using transfer learning and fine-tuning techniques.\n",
        "\n",
        "> <img src=\t\"\thttps://raw.githubusercontent.com/mhrgroup/course_self_supervised_learning/main/images/basemodel.png\t\"\twidth=\"150\"/>\n",
        "\n",
        "* A base model usually includes hundreds of convolutional, recurrent, or dense (i.e., fully connected) layers. These layers have been trained (for days) on a different data set than the current problem using expensive resources. Such datasets might have thousands of categories or include millions of data points.\n",
        "\n",
        "* The base model is enriched with learned parameters, weights, filters, and kernels capable of identifying feature representation for any problem within the same data domain as the one base model was trained on.\n",
        "\n",
        "* The base model’s input and output layers are often modified to be compatible with the current problem. For example, non-trainable lower/upper scaling layers are added to the input layer for input dimension compatibility. Output layer/s are substituted with trainable dense/SoftMax layers compatible with the current dataset’s output.\n",
        "\n",
        "> <img src=\t\"\thttps://raw.githubusercontent.com/mhrgroup/course_self_supervised_learning/main/images/modification.png\t\"\twidth=\"175\"/>\n",
        "\n",
        "* The base model (or a portion of the base model) is frozen in transfer learning, meaning its parameters are not updated. Instead, the newly added output layer is trained using a relatively large learning rate for many epochs.\n",
        "\n",
        "> <img src=\t\"\thttps://raw.githubusercontent.com/mhrgroup/course_self_supervised_learning/main/images/transferlearning.png\t\"\twidth=\"175\"/>\n",
        "\n",
        "* The base model already produced enriched feature representation from data; the only task is to learn from this representation to address the current problem using the newly added output layer.\n",
        "\n",
        "* The base model (or a portion of the base model) is unfrozen in fine-tuning. Its parameters can now be updated using a relatively small learning rate for a few epochs to prevent overfitting, especially if the current dataset has few labeled data. Fine-tuning often boosts the accuracy of the model significantly.\n",
        "\n",
        "> <img src=\t\"\thttps://raw.githubusercontent.com/mhrgroup/course_self_supervised_learning/main/images/finetuning.png\t\"\twidth=\"175\"/>\n",
        "\n",
        "* The whole process is shown in this figure:\n",
        "\n",
        "> <img src=\t\"\thttps://raw.githubusercontent.com/mhrgroup/course_self_supervised_learning/main/images/ssl0.png\t\"\twidth=\"650\"/>\n",
        "\n",
        "* Let's practice this using the CIFAR-10 dataset\n",
        "\n",
        "> **Abbreviations:**\n",
        "*\tdatain: input data\n",
        "*\tdataou: output data\n",
        "*\tte: testing\n",
        "*\ttf: tensorflow\n",
        "*\ttr: training"
      ],
      "metadata": {
        "id": "ynjZiIEM8wN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install cecessary libraries & restart the session\n",
        "\n",
        "# Install the required libraries using the `pip` package manager.\n",
        "!pip install tensorflow==2.15\n",
        "\n",
        "# Import the time module to add a delay before restarting the session.\n",
        "import time\n",
        "\n",
        "# Import `clear_output` from IPython to clear the notebook output, ensuring a clean display for the user.\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Clear the output after the packages are installed to make the notebook cleaner.\n",
        "clear_output()\n",
        "\n",
        "# Print a message to let the user know that the libraries are installed & the session will restart.\n",
        "print(\"Necessary Libraries are Installed. Restarting the session!\")\n",
        "\n",
        "# Add a short delay (1 second) before restarting to allow the message to be displayed to the user.\n",
        "time.sleep(1)\n",
        "\n",
        "# Import the `os` module to access low-level operating system functionality.\n",
        "import os\n",
        "\n",
        "# Use `os._exit(00)` to exit the current Python runtime environment forcefully.\n",
        "# This effectively simulates a restart in notebook environments like Google Colab or Jupyter.\n",
        "# After this command, the environment will be restarted & all the packages installed will be properly loaded.\n",
        "os._exit(00)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dYE4DW_JbG7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import necessary libraries\n",
        "import time\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "pI9Y_6LGYbgb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load and process the CIFAR-10 data\n",
        "(datain_tr, dataou_tr), (datain_te, dataou_te) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "'''\n",
        "List of available datasets at tf.keras.datasets:\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/datasets.\n",
        "'''\n",
        "\n",
        "datain_tr = datain_tr/255 # trasnform unit-8 values between 0 and 1\n",
        "datain_te = datain_te/255 # trasnform unit-8 values between 0 and 1\n",
        "\n",
        "dataou_tr = tf.keras.utils.to_categorical(dataou_tr)\n",
        "dataou_te = tf.keras.utils.to_categorical(dataou_te)\n",
        "\n",
        "# print shapes of data\n",
        "\n",
        "print('Shape of datain_tr: {}'.format(datain_tr.shape))\n",
        "print('Shape of datain_te: {}'.format(datain_te.shape))\n",
        "print('Shape of dataou_tr: {}'.format(dataou_tr.shape))\n",
        "print('Shape of dataou_te: {}'.format(dataou_te.shape))\n"
      ],
      "metadata": {
        "id": "sCPz7gCaWJJx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create a model similar to DenseNet121\n",
        "'''\n",
        "DenseNet121 is a deep neural network tower including convolutional and dense\n",
        "layers. You can learn about it at\n",
        "https://arxiv.org/abs/1608.06993\n",
        "'''\n",
        "\n",
        "layerin = tf.keras.Input(shape=(datain_tr.shape[1],\n",
        "                                datain_tr.shape[2],\n",
        "                                datain_tr.shape[3]))\n",
        "\n",
        "'''\n",
        "The DenseNet121 input size is 160 by 160 by 3, and our images are 32 by 32 by 3.\n",
        "The upscale is to resize our 32 by 32 images to become 160 by 160 for\n",
        "compatibility\n",
        "\n",
        "Read about tf.keras.layers.Lambda here:\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda\n",
        "'''\n",
        "\n",
        "upscale = tf.keras.layers.Lambda(lambda x: tf.image.resize_with_pad(x,\n",
        "                                                                    160,\n",
        "                                                                    160,\n",
        "                                                                    method=tf.image.ResizeMethod.BILINEAR))(layerin)\n",
        "'''\n",
        "Read more about image ResizeMethod at\n",
        "https://www.tensorflow.org/api_docs/python/tf/image/resize\n",
        "'''\n",
        "\n",
        "\n",
        "# Here we are going to use ImageNet weights to create our base model\n",
        "model_base = tf.keras.applications.DenseNet121(include_top=False,\n",
        "                                              weights = 'imagenet',\n",
        "                                              input_shape  = (160,160,3),\n",
        "                                              input_tensor = upscale,\n",
        "                                              pooling='max')\n",
        "\n",
        "'''\n",
        "read more at\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/applications/densenet/DenseNet121\n",
        "'''\n",
        "\n",
        "layerou = tf.keras.layers.Dense(dataou_tr.shape[-1], activation = 'softmax')\n",
        "\n",
        "'''\n",
        "Create the model!\n",
        "We add \"BatchNormalization\" to reduce overfitting.\n",
        "A useful article to learn more about BacthNormalization:\n",
        "https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739\n",
        "'''\n",
        "\n",
        "model   = tf.keras.models.Sequential([model_base,\n",
        "                                      tf.keras.layers.BatchNormalization(),\n",
        "                                      layerou])\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001),\n",
        "              loss      = 'categorical_crossentropy',\n",
        "              metrics   = ['accuracy'])\n",
        "\n",
        "\n",
        "'''\n",
        "Print model architectures\n",
        "'''\n",
        "\n",
        "print('summary of model_DenseNet121:\\n')\n",
        "model_base.summary()\n",
        "\n",
        "print('summary of model:\\n')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "xp0spTuOWJJx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Transfer learning\n",
        "# Freeze base model and train the last layer only\n",
        "model_base.trainable = False\n",
        "\n",
        "'''\n",
        "When you freeze model_base, it is also being frozen within the model; they are\n",
        "all at the exact location on your memory, but you have to compile the model\n",
        "again.\n",
        "'''\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.001),\n",
        "              loss      = 'categorical_crossentropy',\n",
        "              metrics   = ['accuracy'])\n",
        "\n",
        "print('\\nsummary of model_DenseNet121:\\n')\n",
        "model_base.summary()\n",
        "\n",
        "print('\\nsummary of model:\\n')\n",
        "model.summary()\n",
        "\n",
        "'''\n",
        "Train for three epochs\n",
        "'''\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "history = model.fit(datain_tr, dataou_tr, epochs = 3, batch_size = 128,\n",
        "                    verbose = 1, shuffle = True, validation_split = 0.05)\n",
        "\n",
        "t_end   = time.time()\n",
        "\n",
        "t_transfer_learning = t_end - t_start\n",
        "\n",
        "print('\\nTransfer learning training time: {:06.2f} sec'.format(t_transfer_learning))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UFzTkNY-eu0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Fine-tuning\n",
        "\n",
        "# Unfreez the base model and train the last layer only\n",
        "model_base.trainable = True\n",
        "\n",
        "'''\n",
        "When you unfreeze model_base, it is also being unfrozen within the model;\n",
        "they are all at the exact location on your memory, but you have to compile the\n",
        "model again.\n",
        "'''\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(0.00001),\n",
        "              loss      = 'categorical_crossentropy',\n",
        "              metrics   = ['accuracy'])\n",
        "\n",
        "print('summary of model_DenseNet121:\\n')\n",
        "model_base.summary()\n",
        "\n",
        "print('summary of model:\\n')\n",
        "model.summary()\n",
        "\n",
        "'''\n",
        "Train for two epochs\n",
        "'''\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "history = model.fit(datain_tr,\n",
        "                    dataou_tr,\n",
        "                    epochs           = 2,\n",
        "                    batch_size       = 128,\n",
        "                    verbose          = 1,\n",
        "                    shuffle          = True,\n",
        "                    validation_split = 0.05)\n",
        "\n",
        "t_end   = time.time()\n",
        "\n",
        "t_fine_tuning = t_end - t_start\n",
        "\n",
        "print('\\nFine-tuning training time: {:06.2f} sec'.format(t_fine_tuning))\n"
      ],
      "metadata": {
        "id": "5qrDzPCUezUI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compute testing accuracy\n",
        "_, accuracy_te = model.evaluate(datain_te,\n",
        "                                dataou_te,\n",
        "                                batch_size = 128)\n",
        "\n",
        "print('\\nTotal Training time: {:06.2f} sec'.format(t_transfer_learning + t_fine_tuning))\n",
        "print('\\nTesting acuuracy: {:05.2f}%'.format(accuracy_te * 100))"
      ],
      "metadata": {
        "id": "kew4FGaWWJJy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clean up memory\n",
        "%reset"
      ],
      "metadata": {
        "id": "zySh2KaiYD3O",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 04: Transfer Learning and Fine-Tuning**\n",
        "\n",
        "In this lecture, you learned about:\n",
        "\n",
        "1. supervised learning with transfer learning and fine-tuning.\n",
        "\n",
        "> ***In the following lecture, we will discuss \"Labeling Challenges.\"***"
      ],
      "metadata": {
        "id": "OWML14xDnq7v"
      }
    }
  ]
}