{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ssl_section04_lecture07.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bBiYNuP3LRf-"
      ],
      "authorship_tag": "ABX9TyPChXOlDtMKF5OOiiZI2fKP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhrgroup/course_self_supervised_learning/blob/main/Section%2004%3A%20Self-Supervised%20Learning/ssl_section04_lecture07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 07: Supervised Contrastive Pretext, Experiment 1**\n",
        "\n",
        "By the end of this lecture, you will be able to:\n",
        "\n",
        "1. Address a labeling problem with supervised contrastive pretext with randomly generated parameters."
      ],
      "metadata": {
        "id": "Wt9jAeTSiWju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7.1. Experiment 1**\n",
        "---\n",
        "* In this experiment, we assume only 1000 training images are labeled in CIFAR-10. \n",
        "* We develop a supervised contrastive pretext (prx) model using all training inputs and fine-tune it on the 1000 labeled images in the downstream (dwm) task. \n",
        "* We then label the testing images using the fine-tuned model.\n",
        "* We assume that there is no trained network on similar data distribution. Hence, our model does not have pretrained parameters (we have to generate the parameters randomly). \n",
        "* We compare this model with the result of a reasonably similar fully supervised (fsp) model trained on the 1000 labeled data only.\n",
        "* Note: we have to develop three models here, model_fsp, model_prx, and model_dwm.\n",
        "\n",
        "> ***Heads up: we should compare model_fsp and model_dwm accuracies on testing data.***\n",
        "\n",
        "> **Abbreviations:**\n",
        "* acc: accuracy\n",
        "*\tdatain: input data\n",
        "*\tdataou: output data\n",
        "*\tdwm: downstream\n",
        "*\tfnt: fine-tuning\n",
        "*\tfsp: fully supervised learning\n",
        "* lr: learning rate\n",
        "*\tprx: pretext\n",
        "*\tte: testing\n",
        "*\ttf: tensorflow\n",
        "*\ttr: training\n",
        "*\ttrf: transfer learning"
      ],
      "metadata": {
        "id": "SFe73rDdh8yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import copy"
      ],
      "metadata": {
        "id": "FzYfeIUSInUq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyper-parameters\n",
        "num_labeled  = 1000\n",
        "\n",
        "# learning rates\n",
        "lr_fsp       = 0.001\n",
        "lr_prx       = 0.001\n",
        "lr_dwm_trf   = 0.01\n",
        "lr_dwm_fnt   = 0.0001\n",
        "\n",
        "# batch sizes\n",
        "batch_fsp      = 128\n",
        "batch_prx      = 128\n",
        "batch_dwm_trf  = 128\n",
        "batch_dwm_fnt  = 128\n",
        "\n",
        "# epochs\n",
        "'''\n",
        "We keep epoch_dwm_trf + epoch_dwm_fnt = epoch_fsp for a \n",
        "fair comparison.\n",
        "'''\n",
        "epoch_fsp      = 25\n",
        "epoch_prx      = 10\n",
        "epoch_dwm_trf  = 15\n",
        "epoch_dwm_fnt  = 10\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-q5M1UgDqkDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load and process the CIFAR-10 data\n",
        "(datain_tr, dataou_tr), (datain_te, dataou_te) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "datain_tr = datain_tr/255 # trasnform unit-8 values between 0 and 1\n",
        "datain_te = datain_te/255 # trasnform unit-8 values between 0 and 1\n",
        "\n",
        "dataou_tr = tf.keras.utils.to_categorical(dataou_tr)\n",
        "dataou_te = tf.keras.utils.to_categorical(dataou_te)\n",
        "\n",
        "print('Shape of datain_tr: {}'.format(datain_tr.shape))\n",
        "print('Shape of datain_te: {}'.format(datain_te.shape))\n",
        "print('Shape of dataou_tr: {}'.format(dataou_tr.shape))\n",
        "print('Shape of dataou_te: {}'.format(dataou_te.shape))\n"
      ],
      "metadata": {
        "id": "I15ccnhjzDMX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create contrastive training inputs and outputs\n",
        "'''\n",
        "Let's pick an augmentation method, say, random rotation.\n",
        "'''\n",
        "\n",
        "fun_augment     = tf.keras.layers.RandomRotation(factor = 0.2)\n",
        "\n",
        "datain_tr_augmented = fun_augment(datain_tr)\n",
        "\n",
        "#concatenate the original and augmented training data for pretext (prx)\n",
        "datain_tr_prx = tf.concat([datain_tr,datain_tr_augmented], axis = 0)\n",
        "\n",
        "#contrastive outputs\n",
        "dataou_tr_prx_positive = tf.ones((datain_tr.shape[0],1))\n",
        "dataou_tr_prx_negative = tf.zeros((datain_tr_augmented.shape[0],1))\n",
        "\n",
        "#Output concatenation based on the order of data points in datain_tr_prx\n",
        "dataou_tr_prx = tf.concat([dataou_tr_prx_negative, dataou_tr_prx_positive], axis = 0)\n",
        "\n",
        "#binary categorical for SoftMax:\n",
        "dataou_tr_prx = tf.keras.utils.to_categorical(dataou_tr_prx)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zUqp7KD-vmZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Limit the labeled training data\n",
        "\n",
        "# randomly select num_labeled of training data\n",
        "index_tr  = tf.experimental.numpy.random.randint(0, \n",
        "                                                 datain_tr.shape[0], \n",
        "                                                 num_labeled)\n",
        "\n",
        "datain_tr_labeled = datain_tr[index_tr,:,:,:]\n",
        "dataou_tr_labeled = dataou_tr[index_tr,:]\n",
        "\n",
        "'''\n",
        "For readability.\n",
        "'''\n",
        "\n",
        "datain_tr_fsp = copy.deepcopy(datain_tr_labeled)\n",
        "dataou_tr_fsp = copy.deepcopy(dataou_tr_labeled)\n",
        "\n",
        "datain_tr_dwm = copy.deepcopy(datain_tr_labeled)\n",
        "dataou_tr_dwm = copy.deepcopy(dataou_tr_labeled)\n",
        "\n",
        "# we have 50,000 training inputs; num_labeled of them are labeled\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EPB_PDAPURTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create model_fsp and model_dwm similar to DenseNet121\n",
        "\n",
        "layerin = tf.keras.Input(shape=(datain_tr.shape[1], \n",
        "                                datain_tr.shape[2],\n",
        "                                datain_tr.shape[3]))\n",
        "\n",
        "\n",
        "upscale = tf.keras.layers.Lambda(lambda x: tf.image.resize_with_pad(x,\n",
        "                                                                    160,\n",
        "                                                                    160,\n",
        "                                                                    method=tf.image.ResizeMethod.BILINEAR))(layerin)\n",
        "\n",
        "\n",
        "model_DenseNet121 = tf.keras.applications.DenseNet121(include_top  = False,\n",
        "                                                      weights      = None,\n",
        "                                                      input_shape  = (160,160,3),\n",
        "                                                      input_tensor = upscale,\n",
        "                                                      pooling      = 'max')\n",
        "'''\n",
        "We clone model_DenseNet121 to create model_base of our fully supervised model\n",
        "and pretext model.\n",
        "\n",
        "P.S. We clone the downstream model after the pretext task using \n",
        "the pretext model.\n",
        "'''\n",
        "model_base_fsp =  tf.keras.models.clone_model(model_DenseNet121)\n",
        "model_base_prx =  tf.keras.models.clone_model(model_DenseNet121)\n",
        "\n",
        "'''\n",
        "We set the parameters of model_base_fsp and model_base_prx to be the same as the \n",
        "randomly generated parameters of model_DenseNet121 to have both model learning\n",
        "start point the same for a fair comparison.\n",
        "'''\n",
        "\n",
        "model_base_fsp.set_weights(model_DenseNet121.get_weights())\n",
        "model_base_prx.set_weights(model_DenseNet121.get_weights())\n",
        "\n",
        "\n",
        "'''\n",
        "Now we create batch norm layers of model_fsp and model_prx.\n",
        "'''\n",
        "layer_batchnorm_fsp = tf.keras.layers.BatchNormalization()\n",
        "layer_batchnorm_prx = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "'''\n",
        "Now we create output layers of model_fsp and model_prx.\n",
        "'''\n",
        "layerou_fsp = tf.keras.layers.Dense(dataou_tr_fsp.shape[-1], \n",
        "                                    activation = 'softmax')\n",
        "\n",
        "layerou_prx = tf.keras.layers.Dense(dataou_tr_prx.shape[-1], \n",
        "                                    activation = 'softmax')\n",
        "\n",
        "\n",
        "'''\n",
        "Now we create model_fsp and model_prx.\n",
        "'''\n",
        "model_fsp   = tf.keras.models.Sequential([model_base_fsp, \n",
        "                                          layer_batchnorm_fsp, \n",
        "                                          layerou_fsp])\n",
        "\n",
        "model_prx   = tf.keras.models.Sequential([model_base_prx, \n",
        "                                          layer_batchnorm_prx, \n",
        "                                          layerou_prx])\n",
        "\n",
        "model_fsp.compile(optimizer = tf.keras.optimizers.Adam(lr_fsp), \n",
        "                  loss      = 'categorical_crossentropy', \n",
        "                  metrics   = ['accuracy'])\n",
        "\n",
        "model_prx.compile(optimizer = tf.keras.optimizers.Adam(lr_prx), \n",
        "                  loss      = 'categorical_crossentropy', \n",
        "                  metrics   = ['accuracy'])\n",
        "\n",
        "'''\n",
        "Control if model_fsp has similar hyper-parameters as model_prx.\n",
        "\n",
        "P.S. Note that we don't expect the last layers of model_fsp and model_prx to be \n",
        "similar, but later we need to make sure the initial parameters of the last \n",
        "layers of model_dwm are the same as model_fsp.\n",
        "'''\n",
        "\n",
        "print('Base model | fsp: ', model_fsp.layers[0].layers[10].weights[0][0][0][0][:5])\n",
        "print('Base model | prx: ', model_prx.layers[0].layers[10].weights[0][0][0][0][:5])\n",
        "\n",
        "print('Batch normalizaiton layer | fsp: ', model_fsp.layers[1].weights[:2])\n",
        "print('Batch normalizaiton layer | prx: ', model_prx.layers[1].weights[:2])\n",
        "\n",
        "'''\n",
        "Let's keep model_fsp last layer's initial parameters for later use.\n",
        "'''\n",
        "layerou_fsp_initial_parameters = copy.deepcopy(model_fsp.layers[2].weights)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1XMn2AVFy8Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train the fsp model \n",
        "'''\n",
        "We don't include any validation split here since we can verify overfitting \n",
        "existence later using testing data.\n",
        "'''\n",
        "history_fsp = model_fsp.fit(datain_tr_fsp, \n",
        "                            dataou_tr_fsp, \n",
        "                            epochs           = epoch_fsp, \n",
        "                            batch_size       = batch_fsp, \n",
        "                            verbose          = 1, \n",
        "                            shuffle          = True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "K55ixp3vy8Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train the prx model\n",
        "'''\n",
        "Note that we do not have testing pretext data, so we track overfitting by\n",
        "including validation split. Also, since prx takes time, we define some early \n",
        "stopping criteria using \"callbacks.\"\n",
        "'''\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor              = 'val_loss',\n",
        "                                            patience             = 2,\n",
        "                                            restore_best_weights = True)\n",
        "\n",
        "history_prx = model_prx.fit(datain_tr_prx, \n",
        "                            dataou_tr_prx, \n",
        "                            epochs           = epoch_prx, \n",
        "                            batch_size       = batch_prx, \n",
        "                            verbose          = 1, \n",
        "                            shuffle          = True,\n",
        "                            validation_split = 0.05,\n",
        "                            callbacks        = [callback])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cNTAM_ChlbOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Some controls \n",
        "'''\n",
        "Control if model_fsp has similar hyper-parameters as model_prx\n",
        "'''\n",
        "\n",
        "print('Base model | fsp: ', model_fsp.layers[0].layers[10].weights[0][0][0][0][:5])\n",
        "print('Base model | prx: ', model_prx.layers[0].layers[10].weights[0][0][0][0][:5])\n",
        "\n",
        "print('Batch normalizaiton layer | fsp: ', model_fsp.layers[1].weights[:2])\n",
        "print('Batch normalizaiton layer | prx: ', model_prx.layers[1].weights[:2])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Yh1p3IFrmECn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create and train the dwm model using transfer-learning only\n",
        "'''\n",
        "Downstream output layer\n",
        "'''\n",
        "layerou_dwm = tf.keras.layers.Dense(dataou_tr_dwm.shape[-1], \n",
        "                                    activation = 'softmax')\n",
        "\n",
        "'''\n",
        "Now we create model_dwm using model_base_prx (yes, it is trained!), \n",
        "layer_batchnorm_prx (yes, it is trained too!), and layerou_dwm, but for a fair \n",
        "fsp and dwm comparison, we set the dwm parameters to be similar to \n",
        "layerou_fsp_initial_parameters. \n",
        "\n",
        "P.S. Note that at the transfer learning level, we don't update model_base_prx \n",
        "and layer_batchnorm_prx  parameters and focus on learning layerou_dwm only.\n",
        "'''\n",
        "\n",
        "model_base_prx.trainable      = False\n",
        "layer_batchnorm_prx.trainable = False\n",
        "\n",
        "model_dwm   = tf.keras.models.Sequential([model_base_prx, \n",
        "                                          layer_batchnorm_prx, \n",
        "                                          layerou_dwm])\n",
        "\n",
        "\n",
        "model_dwm.layers[2].set_weights(layerou_fsp_initial_parameters)\n",
        "\n",
        "\n",
        "model_dwm.compile(optimizer = tf.keras.optimizers.Adam(lr_dwm_trf), \n",
        "                  loss      = 'categorical_crossentropy', \n",
        "                  metrics   = ['accuracy'])\n",
        "\n",
        "model_dwm.summary()\n",
        "\n",
        "history_dwm = model_dwm.fit(datain_tr_dwm, \n",
        "                            dataou_tr_dwm, \n",
        "                            epochs           = epoch_dwm_trf, \n",
        "                            batch_size       = batch_dwm_trf, \n",
        "                            verbose          = 1, \n",
        "                            shuffle          = True)"
      ],
      "metadata": {
        "id": "za52rJgzm71v",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Fine-tuning model_dwm\n",
        "model_base_prx.trainable      = True\n",
        "layer_batchnorm_prx.trainable = True\n",
        "\n",
        "# We can fine-tune after certain model_base_prx layer!\n",
        "# fine_tune_after = 349\n",
        "# for layer in model_base_prx.layers[:fine_tune_after]:\n",
        "#   layer.trainable = False\n",
        "\n",
        "model_dwm.compile(optimizer = tf.keras.optimizers.Adam(lr_dwm_fnt), \n",
        "                  loss      = 'categorical_crossentropy', \n",
        "                  metrics   = ['accuracy'])\n",
        "\n",
        "model_dwm.summary()\n",
        "\n",
        "history_dwm = model_dwm.fit(datain_tr_dwm, \n",
        "                            dataou_tr_dwm, \n",
        "                            epochs           = epoch_dwm_fnt, \n",
        "                            batch_size       = batch_dwm_fnt, \n",
        "                            verbose          = 1, \n",
        "                            shuffle          = True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uM37dhFKvJtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compute model_fsp and model_dwm testing accuracies\n",
        "_, acc_te_fsp = model_fsp.evaluate(datain_te, \n",
        "                                   dataou_te, \n",
        "                                   batch_size = 128)\n",
        "\n",
        "_, acc_te_dwm = model_dwm.evaluate(datain_te, \n",
        "                                   dataou_te, \n",
        "                                   batch_size = 128)\n",
        "\n",
        "print('Accuracy of fsp: {:05.2f}%'.format(acc_te_fsp*100))\n",
        "print('Accuracy of dwm: {:05.2f}%'.format(acc_te_dwm*100))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5p7gDls-y8Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clean up memory\n",
        "%reset"
      ],
      "metadata": {
        "cellView": "form",
        "id": "moW3GaaXy8Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ***Heads-up: We assumed that there is no trained network on similar data distribution. Hence, our fsp and prx models did not have pretrained parameters (we have to generate the parameters randomly).***"
      ],
      "metadata": {
        "id": "wZb-WpHmIocT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 07: Supervised Contrastive Pretext, Experiment 1**\n",
        "\n",
        "In this lecture, you learned about:\n",
        "\n",
        "1. How to address a labeling problem with supervised contrastive pretext with randomly generated parameters.\n",
        "\n",
        "> ***In the following lecture, we will repeat this labeling example using a supervised contrastive pretext with already-trained parameters.***"
      ],
      "metadata": {
        "id": "bBiYNuP3LRf-"
      }
    }
  ]
}