{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ssl_section04_lecture06.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhrgroup/course_self_supervised_learning/blob/main/Section%2004%3A%20Self-Supervised%20Learning/ssl_section04_lecture06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 06: Self-Supervised Learning**\n",
        "\n",
        "By the end of this lecture, you will be able to:\n",
        "\n",
        "1. Describe Self-Supervised Learning (SSL).\n",
        "2. Describe SSL contrastive learning.\n",
        "3. Describe SSL generative learning."
      ],
      "metadata": {
        "id": "Wt9jAeTSiWju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6.1. Self-Supervised Learning (SSL) in General**\n",
        "---\n",
        "Let’s focus again on the previous example: we have ten million images of cats and dogs, with only 10,000 (0.1%) manually labeled.\n",
        "\n",
        "* ***The solutions mentioned in the previous lecture entirely ignore the unlabeled repository. Although unlabeled, there are 9,990,000 images there to learn from somehow.***\n",
        "\n",
        "\n",
        "* ***The Self-Supervised Learning (SSL) (also known as representation learning) objective is (usually) to implement the repository’s labeled and unlabeled data together to develop a base model in a so-called pretext (prx) task and transfer and fine-tune that base model using the limited labeled data in a so-called downstream (dwm) task.***\n",
        "\n",
        "* ***SSL ensures the base model, often referred to as the pretext model in SSL literature, is within the distribution of the current data repository.***\n",
        "\n",
        "> But how to create a pretext model from both labeled and unlabeled data? Two primary SSL techniques exist to create a pretext model: **contrastive learning** and **generative learning**."
      ],
      "metadata": {
        "id": "SFe73rDdh8yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6.2. Contrastive Learning**\n",
        "----\n",
        "\n",
        "* **In contrastive learning**, the input data, labeled or unlabeled, are augmented automatically using an augmentation technique.\n",
        "* A collection of popular augmentations in the image domain is shown in this figure:\n",
        "\n",
        "> <img src=\t\"https://raw.githubusercontent.com/mhrgroup/course_self_supervised_learning/main/images/augmentation.png\"\twidth=\"650\"/>\n",
        "\n",
        "> Image from [Chen et al. (2020)](http://proceedings.mlr.press/v119/chen20j/chen20j.pdf) (Original image cc-by: Von.grzanka).\n",
        "\n",
        "* First, an augmentation is selected, say, cutout (also known as masking). Next, that augmentation is automatically applied to the input data, labeled or unlabeled.\n",
        "* Hence, twenty million images are retrieved for our repository of cats and dogs: ten million original images and ten million augmented ones, disregard of being a dog or cat.\n",
        "* In other words, there are ten million pairs of original and corresponding augmented images.\n",
        "\n",
        "* A collection of popular augmentations in the signal (temporal records) domain is shown in this figure:\n",
        "\n",
        "> <img src=\t\"https://raw.githubusercontent.com/mhrgroup/course_self_supervised_learning/main/images/signal.png\"\twidth=\"300\"/>\n",
        "\n",
        "> ***Let's code some image augmentations using the CIFAR-10 data***\n",
        "\n",
        "> **Abbreviations:**\n",
        "*\tdatain: input data\n",
        "*\tdataou: output data\n",
        "*\tte: testing\n",
        "*\ttf: tensorflow\n",
        "*\ttr: training\n"
      ],
      "metadata": {
        "id": "2oVKMDTdmNz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install necessary libraries & restart the session\n",
        "\n",
        "# Install the required libraries using the `pip` package manager.\n",
        "!pip install tensorflow==2.15\n",
        "\n",
        "# Import the time module to add a delay before restarting the session.\n",
        "import time\n",
        "\n",
        "# Import `clear_output` from IPython to clear the notebook output, ensuring a clean display for the user.\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Clear the output after the packages are installed to make the notebook cleaner.\n",
        "clear_output()\n",
        "\n",
        "# Print a message to let the user know that the libraries are installed & the session will restart.\n",
        "print(\"Necessary Libraries are Installed. Restarting the session!\")\n",
        "\n",
        "# Add a short delay (1 second) before restarting to allow the message to be displayed to the user.\n",
        "time.sleep(1)\n",
        "\n",
        "# Import the `os` module to access low-level operating system functionality.\n",
        "import os\n",
        "\n",
        "# Use `os._exit(00)` to exit the current Python runtime environment forcefully.\n",
        "# This effectively simulates a restart in notebook environments like Google Colab or Jupyter.\n",
        "# After this command, the environment will be restarted & all the packages installed will be properly loaded.\n",
        "os._exit(00)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "d5GD1cM-bpxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import matplotlib as mt\n"
      ],
      "metadata": {
        "id": "FzYfeIUSInUq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load and process the CIFAR-10 data\n",
        "(datain_tr, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "datain_tr = datain_tr/255 # trasnform unit-8 values between 0 and 1\n",
        "\n",
        "print('Shape of datain_tr: {}'.format(datain_tr.shape))\n"
      ],
      "metadata": {
        "id": "FDhr3sLoInMS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot function for image augmentation\n",
        "def fun_plot(image_original, image_augmented, general_title):\n",
        "  titles = ['original', 'augmented']\n",
        "  images = [image_original, image_augmented]\n",
        "  fig, ax = mt.pyplot.subplots(1,2, figsize=(4,3));\n",
        "  fig.suptitle(general_title.title())\n",
        "\n",
        "  fig.tight_layout()\n",
        "  mt.pyplot.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "\n",
        "  for i0 in range(2):\n",
        "    ax[i0].imshow(images[i0])\n",
        "    ax[i0].set_title(titles[i0].title())\n",
        "    ax[i0].set_xticks([])\n",
        "    ax[i0].set_yticks([])\n",
        "\n",
        "  mt.pyplot.show()\n"
      ],
      "metadata": {
        "id": "U2nMmBEzSENb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image augmentation - random flip\n",
        "'''\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomFlip.\n",
        "'''\n",
        "\n",
        "fun_augment     = tf.keras.layers.RandomFlip(mode = 'horizontal_and_vertical')\n",
        "\n",
        "image_original  = datain_tr[0,:,:,:]\n",
        "image_augmented = fun_augment(image_original)\n",
        "\n",
        "fun_plot(image_original, image_augmented, 'Random Flip')\n"
      ],
      "metadata": {
        "id": "A76WBRnAdV2W",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image augmentation - random rotation\n",
        "'''\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomRotation.\n",
        "'''\n",
        "\n",
        "fun_augment     = tf.keras.layers.RandomRotation(factor = 0.2)\n",
        "\n",
        "image_original  = datain_tr[0,:,:,:]\n",
        "image_augmented = fun_augment(image_original)\n",
        "\n",
        "fun_plot(image_original, image_augmented, 'Random Rotation')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "92jZEZ9RIm-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image augmentation - random zoom\n",
        "'''\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomZoom.\n",
        "'''\n",
        "\n",
        "fun_augment     = tf.keras.layers.RandomZoom(height_factor = .75)\n",
        "\n",
        "image_original  = datain_tr[0,:,:,:]\n",
        "image_augmented = fun_augment(image_original)\n",
        "\n",
        "fun_plot(image_original, image_augmented, 'Random Zoom')\n"
      ],
      "metadata": {
        "id": "eq6xO3aJKqwY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image augmentation - random crop and resize\n",
        "'''\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomCrop.\n",
        "'''\n",
        "\n",
        "fun_augment_01  = tf.keras.layers.RandomCrop(height = 20, width = 20)\n",
        "fun_augment_02  = tf.keras.layers.Resizing(height = datain_tr.shape[1],\n",
        "                                           width = datain_tr.shape[2])\n",
        "\n",
        "fun_augment     = tf.keras.Sequential([fun_augment_01, fun_augment_02])\n",
        "\n",
        "image_original  = datain_tr[0,:,:,:]\n",
        "image_augmented = fun_augment(image_original)\n",
        "\n",
        "fun_plot(image_original, image_augmented, 'Crop & Resize')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oTc3Ku6mKqt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image augmentation - random saturation (jitter)\n",
        "'''\n",
        "https://www.tensorflow.org/api_docs/python/tf/image/random_saturation.\n",
        "'''\n",
        "\n",
        "fun_augment     = lambda image: tf.cast(tf.image.random_saturation(image*255, lower = 1, upper = 5), dtype = tf.float32)/255\n",
        "image_original  = datain_tr[0,:,:,:]\n",
        "\n",
        "image_augmented = fun_augment(image_original)\n",
        "\n",
        "fun_plot(image_original, image_augmented, 'Random Saturation (Jitter)')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hf7sewwPYMKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create contrastive training inputs\n",
        "'''\n",
        "Let's pick an augmentation method, say, random rotation.\n",
        "'''\n",
        "\n",
        "fun_augment     = tf.keras.layers.RandomRotation(factor = 0.2)\n",
        "\n",
        "datain_tr_augmented = fun_augment(datain_tr)\n",
        "\n",
        "# Let's plot the first five augmented images\n",
        "for i0 in range(5):\n",
        "  fun_plot(datain_tr[i0,:,:,:], datain_tr_augmented[i0,:,:,:], 'Random Rotation')\n",
        "\n",
        "# concatenate the original and augmented training data for pretext (prx)\n",
        "datain_tr_prx = tf.concat([datain_tr,datain_tr_augmented], axis = 0)\n",
        "\n",
        "print(\"Shape of original training inputs: {}\".format(datain_tr.shape))\n",
        "print(\"Shape of augmented training inputs: {}\".format(datain_tr_augmented.shape))\n",
        "print(\"Shape of pretext inputs: {}\".format(datain_tr_prx.shape))\n"
      ],
      "metadata": {
        "id": "aztzeNzdhiqN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2.1. Supervised Contrastive Learning\n",
        "* In **supervised contrastive learning**, an augmented image is pseudo-labeled positive (class 1), and the original image is pseudo-labeled negative (class 0).\n",
        "* The pretext task’s goal is a pretext model to learn these pseudo-labels using all two-million images. Next, the pretext model is transferred and fine-tuned using the limited labeled data in the downstream task."
      ],
      "metadata": {
        "id": "6waXL7kcfApO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create pseudo labels\n",
        "dataou_tr_prx_positive = tf.ones((datain_tr.shape[0],1))\n",
        "dataou_tr_prx_negative = tf.zeros((datain_tr_augmented.shape[0],1))\n",
        "\n",
        "#based on the order of data points in datain_tr_prx\n",
        "dataou_tr_prx = tf.concat([dataou_tr_prx_negative, dataou_tr_prx_positive], axis = 0)\n",
        "\n",
        "#binary categorical for SoftMax:\n",
        "dataou_tr_prx = tf.keras.utils.to_categorical(dataou_tr_prx)\n",
        "\n",
        "print(\"Shape of training pretext output: {}\".format(dataou_tr_prx.shape))"
      ],
      "metadata": {
        "id": "vOnJq2jrIpBa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2.1. Unsupervised Contrastive Learning\n",
        "* In **unsupervised contrastive learning**, no pseudo-labeling is required.\n",
        "* The pretext task’s goal is a pretext model to learn the contrast between feature representations of the twenty million images.\n",
        "* Feature representations are usually retrieved from the pretext model’s last (or near-final) dense layers.\n",
        "* For example, a pretext model’s loss function is to proximate feature representations of an original image and its augmentation in a training batch while marginalizing them from feature representations of other data points in the training batch.\n",
        "* Here is the unsupervised contrastive SSL of [Chen et al. 2020](http://proceedings.mlr.press/v119/chen20j/chen20j.pdf), called simCLR:\n",
        "\n",
        "> <img src=\t\"https://raw.githubusercontent.com/mhrgroup/course_self_supervised_learning/main/images/simclr1.png\"\twidth=\"500\"/>"
      ],
      "metadata": {
        "id": "pUFMQsNdIZ92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6.3. Generative Learning**\n",
        "* In **generative SSL**, a pretext model is retrieved from an unsupervised model, often with no augmentation.\n",
        "* For example, the pretext task is to train a generative adversarial network (GAN) to generate synthetic input data (e.g., images of cats and dogs) near the distribution of the current repository.\n",
        "* Next, the GAN’s discriminator is transferred and fine-tuned using the limited labeled data in the downstream task.\n",
        "* Another is extracting an encoder from an autoencoder that precisely reconstructs the current input distribution to be transferred and fine-tuned using the limited labeled data in the downstream task.\n",
        "\n",
        "> <img src=\t\"https://raw.githubusercontent.com/mhrgroup/course_self_supervised_learning/main/images/gan.png\"\twidth=\"650\"/>\n",
        "\n",
        "> ***In this course, our focus is only on contrastive learning; by generalizing how to develop a contrastive learning pretext model, implementing generative SSL techniques becomes relatively easy.***"
      ],
      "metadata": {
        "id": "5Xe13Tl6ma5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 06: Self-Supervised Learning**\n",
        "\n",
        "In this lecture, you learned about:\n",
        "\n",
        "1. Self-Supervised Learning (SSL).\n",
        "2. Contrastive learning.\n",
        "3. Generative learning.\n",
        "\n",
        "***In the following lecture, we will see a labeling example using supervised contrastive learning.***"
      ],
      "metadata": {
        "id": "bBiYNuP3LRf-"
      }
    }
  ]
}