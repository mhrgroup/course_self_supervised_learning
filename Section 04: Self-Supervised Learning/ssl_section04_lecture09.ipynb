{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ssl_section04_lecture09.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbWcdzxizfUsCa/Fy8beaJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhrgroup/course_self_supervised_learning/blob/main/Section%2004%3A%20Self-Supervised%20Learning/ssl_section04_lecture09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 09: SimCLR, An UnSupervised Contrastive Pretext Model**\n",
        "\n",
        "By the end of this lecture, you will be able to:\n",
        "\n",
        "1. Describe how SimCLR of [Chen et al. (2020)](http://proceedings.mlr.press/v119/chen20j/chen20j.pdf) works. \n",
        "2. Develop SimCLR custom loss and training functions."
      ],
      "metadata": {
        "id": "Wt9jAeTSiWju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9.1. Pretext SimCLR**\n",
        "---\n",
        "* [Chen et al. (2020)](http://proceedings.mlr.press/v119/chen20j/chen20j.pdf) proposed a machine learning framework model for contrastive learning of visual representation (SimCLR). \n",
        "* This unsupervised contrastive model showed a notable accuracy boost on various standard visual data benchmarks such as CIFAR10, CIFAR100, and Caltech-101 after fine-tuning. \n",
        "* SimCLR model is composed of a multi-layer encoder, also referred to as the regressor, and a usually shallow neural network (i.e., dense layers) projector.\n",
        "* The encoder is later transferred for the downstream task.\n",
        "* The SimCLR model enriches pretext models by maximizing the agreement between pairs of augmentations’ representations and marginalizing the representations from augmentations of other data points in a batch. \n",
        "* The idea is not limited to visual data and can be applied to data types such as temporal records. \n",
        "* The following Figure shows the [Chen et al. (2020)](http://proceedings.mlr.press/v119/chen20j/chen20j.pdf) SimCLR pretext model along with a downstream classification model. \n",
        "\n",
        "> <img src=\t\"https://raw.githubusercontent.com/mhrgroup/course_self_supervised_learning/main/images/simclr1.png\"\twidth=\"500\"/>\n",
        "\n",
        "\n",
        "* The SimCLR superiority is because of \n",
        " * Using two augmentations instead of one to retrieve enricher feature representations in the encoder (last encoder’s hidden layer in Figure), and\n",
        " * Using a simple projector to obtain a profound transformation of the encoder’s representation for pretext task. \n",
        "\n",
        "* In a training batch, for a particular data point, the SimCLR’s loss function increases augmentation representations’ proximity while marginalizing these representations from other data points’ augmentation's representations. \n",
        "\n",
        "> <img src=\t\"https://raw.githubusercontent.com/mhrgroup/course_self_supervised_learning/main/images/simclralgorithm0.png\"\twidth=\"400\"/>\n"
      ],
      "metadata": {
        "id": "KYputyuB5QCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9.2. SimCLR Loss Loss and Training Functions**\n",
        "\n",
        "---\n",
        "\n",
        "* We first create a custom loss function.\n",
        "* Next, we create a SimCLR training function.\n"
      ],
      "metadata": {
        "id": "wFx-rm9eemuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.2.1. Loss Function**\n",
        "\n",
        "* Custom loss functions in TensorFlow require two inputs, real and estimated output values. \n",
        "\n",
        "* SimCLR is an unsupervised model, so we create some dummy real output values with the same shape as the predicted ones.  \n",
        "\n",
        "* The estimated output values are the projector's representation, denoted as z_estimate.\n",
        "\n",
        "* Remember if N is the batch size the size of matrix z_real is 2*N.\n",
        "\n",
        "* Our goal is to avoid \"for loops\" in loss and benefit from multi-threading and memory for training speed. \n",
        "\n",
        "* The custom loss functions should be written in the tensor format using the TensorFlow library.\n",
        "\n",
        "> **Abbreviations:**\n",
        "*\tdatain: input data\n",
        "*\tind: index\n",
        "* tf: tensorflow"
      ],
      "metadata": {
        "id": "z_6chqYEw0jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import necessary libraries\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "0XLELvR3oSeI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Custom loss function\n",
        "@tf.function\n",
        "def fun_simclr_loss(z_real, z_estimate):\n",
        "  # z_real is just some dummy variable here \n",
        "  del z_real\n",
        "\n",
        "  # Temperature parameter, which is a hyper-parameter to be optimized for a \n",
        "  # particular problem \n",
        "  toe = .1 \n",
        "\n",
        "  num = int(z_estimate.shape[0]) #num = 2N\n",
        "\n",
        "  # Create i & j indices against each other\n",
        "  ind0 = tf.repeat(tf.expand_dims(tf.range(0,num),axis = 0),num, axis = 0)\n",
        "  ind1 = tf.reshape(ind0, (num**2,1))[:,0]\n",
        "  ind2 = tf.reshape(tf.transpose(ind0), (num**2,1))[:,0]\n",
        "\n",
        "  del ind0\n",
        "\n",
        "  # Arrange the z_estimate values based on ind1\n",
        "  vector_1   = tf.gather(z_estimate, ind1, axis = 0)\n",
        "  del ind1\n",
        "\n",
        "  # Arrange the z_estimate values based on ind2\n",
        "  vector_2   = tf.gather(z_estimate, ind2, axis = 0)\n",
        "  del ind2\n",
        "\n",
        "  # Compute the cosine similarity of vector_1 and vector_2\n",
        "  s      = - tf.reshape(tf.keras.losses.cosine_similarity(vector_1, vector_2, axis=1),(num,num))\n",
        "\n",
        "  del vector_1 \n",
        "  del vector_2\n",
        "\n",
        "  # Compute the nominator of l(i,j)\n",
        "  nom    = tf.exp(s/toe)\n",
        "\n",
        "  # Compute the denominator of l(i,j)\n",
        "  x1    = tf.exp(s/toe)\n",
        "\n",
        "  del s\n",
        "\n",
        "  x2    = 1-tf.eye(num, dtype = tf.float32)\n",
        "\n",
        "  \n",
        "  denom = tf.repeat(tf.expand_dims(tf.math.reduce_sum(x1 * x2, axis = 1), axis = 1), num, axis = 1)\n",
        "  \n",
        "  del x1\n",
        "  del x2\n",
        "\n",
        "  # Compute l(i,j) for all i and j\n",
        "  l     = -tf.math.log(nom/denom)\n",
        "\n",
        "  del nom\n",
        "  del denom \n",
        "\n",
        "  # Compute L\n",
        "  ind_2k0 = tf.range(0,num,2, dtype=tf.int32) \n",
        "  ind_2k1 = tf.range(1,num,2, dtype=tf.int32)\n",
        "\n",
        "  loss_mat1_1 = tf.gather(l,           ind_2k0, axis = 0)\n",
        "  loss_mat1_2 = tf.gather(loss_mat1_1, ind_2k1, axis = 1)\n",
        "  loss_mat1   = tf.linalg.diag_part(loss_mat1_2)\n",
        "\n",
        "  loss_mat2_1 = tf.gather(l,           ind_2k1, axis = 0)\n",
        "  loss_mat2_2 = tf.gather(loss_mat2_1, ind_2k0, axis = 1)\n",
        "  loss_mat2   = tf.linalg.diag_part(loss_mat2_2)\n",
        "\n",
        "  del l\n",
        "\n",
        "  loss_mat = loss_mat1 + loss_mat2\n",
        "\n",
        "  L = tf.math.reduce_sum(loss_mat)/num\n",
        "\n",
        "  return L"
      ],
      "metadata": {
        "id": "KYASeSUfoX_x",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SimCLR training function\n",
        "def fun_train_simclr(model, datain, fun_augment_01, fun_augment_02, \n",
        "                     epochs = 100, batch_size = 32, verbose = 1, \n",
        "                     shuffle = True, patience = 3):\n",
        "  \n",
        "  num_data  = datain.shape[0]\n",
        "  num_batch = int(num_data//batch_size) # Reminder: // is divide and floor\n",
        "  z_size    = model.layers[-1].weights[-1].shape[0]\n",
        "  loss      = []  \n",
        "\n",
        "  for i0 in range(epochs):\n",
        "    counter    = 0\n",
        "    loss_batch = []\n",
        "\n",
        "    if shuffle:\n",
        "      ind_shuffle = tf.experimental.numpy.random.randint(0,datain.shape[0],datain.shape[0])\n",
        "      datain = datain[ind_shuffle,:]\n",
        "\n",
        "\n",
        "    for i1 in range(num_batch):\n",
        "      if i1 == num_batch - 1:\n",
        "        ind_case = range(counter, num_data)\n",
        "      else:\n",
        "        ind_case = range(counter, counter + batch_size)\n",
        "\n",
        "\n",
        "      x_tilda_01  = fun_augment_01(datain[ind_case,:])\n",
        "      x_tilda_02  = fun_augment_02(datain[ind_case,:]) \n",
        "\n",
        "      x_tilda     = tf.reshape(tf.concat([x_tilda_01,x_tilda_02], axis = 1), \n",
        "                               (x_tilda_01.shape[0] + x_tilda_02.shape[0],  \n",
        "                                x_tilda_01.shape[1], x_tilda_01.shape[2], \n",
        "                                x_tilda_01.shape[3]))\n",
        "      \n",
        "      z_real      = tf.random.uniform((x_tilda.shape[0],z_size)) # dummy variable\n",
        "\n",
        "      # Train on batch\n",
        "      var = model.train_on_batch(x_tilda, z_real);\n",
        "      loss_batch.append(var[0]) \n",
        "      # you may change this to loss_batch.append(var) and exclude any 'metrics'\n",
        "      # when compiling the model\n",
        "\n",
        "\n",
        "      counter  = counter + batch_size \n",
        "\n",
        "      if verbose:\n",
        "        if i1 == num_batch - 1:\n",
        "          print(\"\\r SimCLR | Epoch {:04d}/{:04d} - Batch {:04d}/{:04d} - Loss {:8.5F}\".format(i0+1, epochs, i1+1, num_batch, sum(loss_batch)/len(loss_batch)), flush=True)\n",
        "        else:\n",
        "          print(\"\\r SimCLR | Epoch {:04d}/{:04d} - Batch {:04d}/{:04d} - Loss {:8.5F}\".format(i0+1, epochs, i1+1, num_batch, sum(loss_batch)/len(loss_batch)), end=\"\", flush=True)\n",
        "\n",
        "      \n",
        "\n",
        "    loss.append(sum(loss_batch)/len(loss_batch))\n",
        "\n",
        "    if i0>patience:\n",
        "      loss_hist_min = min(loss)\n",
        "\n",
        "      if loss[-1] > loss_hist_min:\n",
        "        break\n",
        "  \n",
        "  return model, loss"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KydJ9JczvNvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 09: SimCLR, An UnSupervised Contrastive Model**\n",
        "\n",
        "In this lecture, you learned about:\n",
        "\n",
        "1. How SimCLR of [Chen et al. (2020)](http://proceedings.mlr.press/v119/chen20j/chen20j.pdf) works. \n",
        "2. SimCLR custom loss and training functions.\n",
        "\n",
        "> ***In the following lecture, we will see a labeling example using SimCLR unsupervised contrastive learning.*** "
      ],
      "metadata": {
        "id": "bBiYNuP3LRf-"
      }
    }
  ]
}