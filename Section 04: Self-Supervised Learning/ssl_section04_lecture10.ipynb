{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ssl_section04_lecture10.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNJgdM+Npxjm7WcWYPUjRbM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhrgroup/course_self_supervised_learning/blob/main/Section%2004%3A%20Self-Supervised%20Learning/ssl_section04_lecture10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 10: SimCLR Experiment**\n",
        "\n",
        "By the end of this lecture, you will be able to:\n",
        "\n",
        "1. Address a labeling problem with SimCLR using a pretrained encoder."
      ],
      "metadata": {
        "id": "Wt9jAeTSiWju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10.1. Experiment**\n",
        "---\n",
        "* This experiment is similar to the previous ones too!\n",
        "* We assume only 1000 training images are labeled in CIFAR-10. \n",
        "* We develop a SimCLR pretext (prx) model using all training inputs and fine-tune it on the 1000 labeled images in the downstream (dwm) task. \n",
        "* We then label the testing images using the fine-tuned model.\n",
        "* We assume that, there is a trained encoder (regressor) on similar data distribution. Hence, our model has pretrained parameters. \n",
        "* We compare this model with the result of a fairly similar fully supervised (fsp) model trained on the 1000 labeled data.\n",
        "* Note: we have to develop three models here, model_fsp, model_prx, and model_dwm.\n",
        "\n",
        "> **Abbreviations:**\n",
        "* acc: accuracy\n",
        "*\tdatain: input data\n",
        "*\tdataou: output data\n",
        "*\tdwm: downstream\n",
        "*\tfnt: fine-tuning\n",
        "*\tfsp: fully supervised learning\n",
        "* ind:index\n",
        "* lr: learning rate\n",
        "*\tprx: pretext\n",
        "*\tte: testing\n",
        "*\ttf: tensorflow\n",
        "*\ttr: training\n",
        "*\ttrf: transfer learning"
      ],
      "metadata": {
        "id": "SFe73rDdh8yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import necessary libraries\n",
        "!pip install tensorflow==2.8.2\n",
        "\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "import warnings\n",
        "\n",
        "from IPython.display import clear_output \n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "FzYfeIUSInUq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyper-parameters\n",
        "num_labeled  = 1000\n",
        "\n",
        "# learning rates\n",
        "lr_fsp_trf   = 0.01\n",
        "lr_fsp_fnt   = 0.0001\n",
        "\n",
        "lr_prx_trf   = 0.01\n",
        "lr_prx_fnt   = 0.000001\n",
        "\n",
        "lr_dwm_trf   = 0.01\n",
        "lr_dwm_fnt   = 0.0001\n",
        "\n",
        "\n",
        "# batch sizes\n",
        "batch_fsp_trf  = 128\n",
        "batch_fsp_fnt  = 128\n",
        "\n",
        "batch_prx_trf  = 64\n",
        "batch_prx_fnt  = 64\n",
        "\n",
        "batch_dwm_trf  = 128\n",
        "batch_dwm_fnt  = 128\n",
        "\n",
        "\n",
        "# epochs\n",
        "epoch_fsp_trf  = 15\n",
        "epoch_fsp_fnt  = 10 \n",
        "\n",
        "epoch_prx_trf  = 15\n",
        "epoch_prx_fnt  = 10\n",
        "\n",
        "epoch_dwm_trf  = 15\n",
        "epoch_dwm_fnt  = 10\n"
      ],
      "metadata": {
        "id": "-q5M1UgDqkDY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load and process the CIFAR-10 data\n",
        "(datain_tr, dataou_tr), (datain_te, dataou_te) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "datain_tr = datain_tr/255 # trasnform unit-8 values between 0 and 1\n",
        "datain_te = datain_te/255 # trasnform unit-8 values between 0 and 1\n",
        "\n",
        "dataou_tr = tf.keras.utils.to_categorical(dataou_tr)\n",
        "dataou_te = tf.keras.utils.to_categorical(dataou_te)\n",
        "\n",
        "print('Shape of datain_tr: {}'.format(datain_tr.shape))\n",
        "print('Shape of datain_te: {}'.format(datain_te.shape))\n",
        "print('Shape of dataou_tr: {}'.format(dataou_tr.shape))\n",
        "print('Shape of dataou_te: {}'.format(dataou_te.shape))\n"
      ],
      "metadata": {
        "id": "I15ccnhjzDMX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pick two augmentation functions\n",
        "\n",
        "# Crop and Resize\n",
        "fun_augment_a  = tf.keras.layers.RandomCrop(height = 20, width = 20)\n",
        "fun_augment_b  = tf.keras.layers.Resizing(height = datain_tr.shape[1], \n",
        "                                          width = datain_tr.shape[2])\n",
        "\n",
        "fun_augment_01 = tf.keras.Sequential([fun_augment_a, fun_augment_b])\n",
        "\n",
        "# Random rotation\n",
        "fun_augment_02     = tf.keras.layers.RandomRotation(factor = 0.2)"
      ],
      "metadata": {
        "id": "zUqp7KD-vmZP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Limit the labeled training data\n",
        "\n",
        "# Randomly select num_labeled of training data\n",
        "index_tr = tf.experimental.numpy.random.randint(0, \n",
        "                                                datain_tr.shape[0], \n",
        "                                                num_labeled)\n",
        "\n",
        "datain_tr_labeled = datain_tr[index_tr,:,:,:]\n",
        "dataou_tr_labeled = dataou_tr[index_tr,:]\n",
        "\n",
        "datain_tr_fsp = copy.deepcopy(datain_tr_labeled)\n",
        "dataou_tr_fsp = copy.deepcopy(dataou_tr_labeled)\n",
        "\n",
        "datain_tr_prx = copy.deepcopy(datain_tr)\n",
        "\n",
        "datain_tr_dwm = copy.deepcopy(datain_tr_labeled)\n",
        "dataou_tr_dwm = copy.deepcopy(dataou_tr_labeled)\n",
        "\n",
        "# We have 50,000 training inputs; num_labeled of them are labeled\n"
      ],
      "metadata": {
        "id": "EPB_PDAPURTG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Custom loss function\n",
        "@tf.function\n",
        "def fun_simclr_loss(z_real, z_estimate):\n",
        "  # z_real is just some dummy variable here \n",
        "  del z_real\n",
        "\n",
        "  # Temperature parameter, which is a hyper-parameter to be optimized for a \n",
        "  # particular problem \n",
        "  toe = .1 \n",
        "\n",
        "  num = int(z_estimate.shape[0]) #num = 2N\n",
        "\n",
        "  # Create i & j indices against each other\n",
        "  ind0 = tf.repeat(tf.expand_dims(tf.range(0,num),axis = 0),num, axis = 0)\n",
        "  ind1 = tf.reshape(ind0, (num**2,1))[:,0]\n",
        "  ind2 = tf.reshape(tf.transpose(ind0), (num**2,1))[:,0]\n",
        "\n",
        "  del ind0\n",
        "\n",
        "  # Arrange the z_estimate values based on ind1\n",
        "  vector_1   = tf.gather(z_estimate, ind1, axis = 0)\n",
        "  del ind1\n",
        "\n",
        "  # Arrange the z_estimate values based on ind2\n",
        "  vector_2   = tf.gather(z_estimate, ind2, axis = 0)\n",
        "  del ind2\n",
        "\n",
        "  # Compute the cosine similarity of vector_1 and vector_2\n",
        "  s      = - tf.reshape(tf.keras.losses.cosine_similarity(vector_1, \n",
        "                                                          vector_2, \n",
        "                                                          axis=1),(num,num))\n",
        "\n",
        "  del vector_1 \n",
        "  del vector_2\n",
        "\n",
        "  # Compute the nominator of l(i,j)\n",
        "  nom    = tf.exp(s/toe)\n",
        "\n",
        "  # Compute the denominator of l(i,j)\n",
        "  x1    = tf.exp(s/toe)\n",
        "\n",
        "  del s\n",
        "\n",
        "  x2    = 1-tf.eye(num, dtype = tf.float32)\n",
        "\n",
        "  \n",
        "  denom = tf.repeat(tf.expand_dims(tf.math.reduce_sum(x1 * x2, axis = 1), axis = 1), num, axis = 1)\n",
        "  \n",
        "  del x1\n",
        "  del x2\n",
        "\n",
        "  # Compute l(i,j) for all i and j\n",
        "  l     = -tf.math.log(nom/denom)\n",
        "\n",
        "  del nom\n",
        "  del denom \n",
        "\n",
        "  # Compute L\n",
        "  ind_2k0 = tf.range(0,num,2, dtype=tf.int32) \n",
        "  ind_2k1 = tf.range(1,num,2, dtype=tf.int32)\n",
        "\n",
        "  loss_mat1_1 = tf.gather(l,           ind_2k0, axis = 0)\n",
        "  loss_mat1_2 = tf.gather(loss_mat1_1, ind_2k1, axis = 1)\n",
        "  loss_mat1   = tf.linalg.diag_part(loss_mat1_2)\n",
        "\n",
        "  loss_mat2_1 = tf.gather(l,           ind_2k1, axis = 0)\n",
        "  loss_mat2_2 = tf.gather(loss_mat2_1, ind_2k0, axis = 1)\n",
        "  loss_mat2   = tf.linalg.diag_part(loss_mat2_2)\n",
        "\n",
        "  del l\n",
        "\n",
        "  loss_mat = loss_mat1 + loss_mat2\n",
        "\n",
        "  L = tf.math.reduce_sum(loss_mat)/num\n",
        "\n",
        "  return L"
      ],
      "metadata": {
        "id": "zIKranjD3JY4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SimCLR training function\n",
        "def fun_train_simclr(model, datain, fun_augment_01, fun_augment_02, \n",
        "                     epochs = 100, batch_size = 32, verbose = 1, \n",
        "                     shuffle = True, patience = 3):\n",
        "  \n",
        "  num_data  = datain.shape[0]\n",
        "  num_batch = int(num_data//batch_size) # Reminder: // is divide and floor\n",
        "  z_size    = model.layers[-1].weights[-1].shape[0]\n",
        "  loss      = []  \n",
        "\n",
        "  for i0 in range(epochs):\n",
        "    counter    = 0\n",
        "    loss_batch = []\n",
        "\n",
        "    if shuffle:\n",
        "      ind_shuffle = tf.experimental.numpy.random.randint(0,datain.shape[0],datain.shape[0])\n",
        "      datain = datain[ind_shuffle,:]\n",
        "\n",
        "\n",
        "    for i1 in range(num_batch):\n",
        "      if i1 == num_batch - 1:\n",
        "        ind_case = range(counter, num_data)\n",
        "      else:\n",
        "        ind_case = range(counter, counter + batch_size)\n",
        "\n",
        "\n",
        "      x_tilda_01  = fun_augment_01(datain[ind_case,:])\n",
        "      x_tilda_02  = fun_augment_02(datain[ind_case,:]) \n",
        "\n",
        "      x_tilda     = tf.reshape(tf.concat([x_tilda_01,x_tilda_02], axis = 1), \n",
        "                               (x_tilda_01.shape[0] + x_tilda_02.shape[0],  \n",
        "                                x_tilda_01.shape[1], \n",
        "                                x_tilda_01.shape[2], \n",
        "                                x_tilda_01.shape[3]))\n",
        "      \n",
        "      z_real      = tf.random.uniform((x_tilda.shape[0],z_size)) # dummy variable\n",
        "\n",
        "      # Train on batch\n",
        "      var = model.train_on_batch(x_tilda, z_real);\n",
        "      loss_batch.append(var[0])\n",
        "      # you may change this to loss_batch.append(var) and exclude any 'metrics'\n",
        "      # when compiling the model\n",
        "\n",
        "      counter  = counter + batch_size \n",
        "\n",
        "      if verbose:\n",
        "        if i1 == num_batch - 1:\n",
        "          print(\"\\r SimCLR | Epoch {:04d}/{:04d} - Batch {:04d}/{:04d} - Loss {:8.5F}\".format(i0+1, epochs, i1+1, num_batch, sum(loss_batch)/len(loss_batch)), flush=True)\n",
        "        else:\n",
        "          print(\"\\r SimCLR | Epoch {:04d}/{:04d} - Batch {:04d}/{:04d} - Loss {:8.5F}\".format(i0+1, epochs, i1+1, num_batch, sum(loss_batch)/len(loss_batch)), end=\"\", flush=True)\n",
        "\n",
        "      \n",
        "\n",
        "    loss.append(sum(loss_batch)/len(loss_batch))\n",
        "\n",
        "    if i0>patience:\n",
        "      loss_hist_min = min(loss)\n",
        "\n",
        "      if loss[-1] > loss_hist_min:\n",
        "        break\n",
        "  \n",
        "  return model, loss"
      ],
      "metadata": {
        "id": "FFiElm3x3JRW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create model_fsp and model_dwm similar to DenseNet121\n",
        "\n",
        "layerin = tf.keras.Input(shape=(datain_tr.shape[1], \n",
        "                                datain_tr.shape[2],\n",
        "                                datain_tr.shape[3]))\n",
        "\n",
        "upscale = tf.keras.layers.Lambda(lambda x: tf.image.resize_with_pad(x,\n",
        "                                                                    160,\n",
        "                                                                    160,\n",
        "                                                                    method=tf.image.ResizeMethod.BILINEAR))(layerin)\n",
        "\n",
        "model_DenseNet121 = tf.keras.applications.DenseNet121(include_top  = False,\n",
        "                                                      weights      = \"imagenet\",\n",
        "                                                      input_shape  = (160,160,3),\n",
        "                                                      input_tensor = upscale,\n",
        "                                                      pooling      = 'max')\n",
        "\n",
        "model_base_fsp =  tf.keras.models.clone_model(model_DenseNet121)\n",
        "model_base_prx =  tf.keras.models.clone_model(model_DenseNet121) # encoder\n",
        "\n",
        "model_base_fsp.set_weights(model_DenseNet121.get_weights())\n",
        "model_base_prx.set_weights(model_DenseNet121.get_weights())\n",
        "\n",
        "\n",
        "layer_batchnorm_fsp = tf.keras.layers.BatchNormalization()\n",
        "layer_batchnorm_prx = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "'''\n",
        "Now we create the SimCLR projector.\n",
        "'''\n",
        "\n",
        "layers_dense_prx = [tf.keras.Input(shape=(1024)),\n",
        "                    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "                    tf.keras.layers.Dense(128, activation = 'relu')]\n",
        "\n",
        "model_projector = tf.keras.Sequential(layers_dense_prx)\n",
        "\n",
        "'''\n",
        "Now we create output layers of model_fsp.\n",
        "'''\n",
        "\n",
        "layerou_fsp = tf.keras.layers.Dense(dataou_tr_fsp.shape[-1], activation = 'softmax')\n",
        "#layerou_prx = tf.keras.layers.Dense(dataou_tr_prx.shape[-1], activation = 'softmax')\n",
        "\n",
        "\n",
        "'''\n",
        "Now we create model_fsp and model_prx.\n",
        "'''\n",
        "\n",
        "model_fsp   = tf.keras.models.Sequential([model_base_fsp, \n",
        "                                          layer_batchnorm_fsp, \n",
        "                                          layerou_fsp])\n",
        "\n",
        "model_prx   = tf.keras.models.Sequential([model_base_prx, \n",
        "                                          layer_batchnorm_prx, \n",
        "                                          model_projector])\n"
      ],
      "metadata": {
        "id": "1XMn2AVFy8Rg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train the fsp model using transfer learning and fine-tuning\n",
        "\n",
        "# Transfer learning\n",
        "model_base_fsp.trainable      = False\n",
        "layer_batchnorm_fsp.trainable = False\n",
        "\n",
        "model_fsp.compile(optimizer = tf.keras.optimizers.Adam(lr_fsp_trf), \n",
        "                  loss      = 'categorical_crossentropy', \n",
        "                  metrics   = ['accuracy'])\n",
        "\n",
        "layerou_fsp_initial_parameters = copy.deepcopy(model_fsp.layers[2].weights)\n",
        "\n",
        "model_fsp.summary()\n",
        "\n",
        "history_fsp_trf = model_fsp.fit(datain_tr_fsp, \n",
        "                                dataou_tr_fsp, \n",
        "                                epochs           = epoch_fsp_trf, \n",
        "                                batch_size       = batch_fsp_trf, \n",
        "                                verbose          = 1, \n",
        "                                shuffle          = True)\n",
        "\n",
        "# Fine-tuning\n",
        "\n",
        "model_base_fsp.trainable      = True\n",
        "layer_batchnorm_fsp.trainable = True\n",
        "\n",
        "model_fsp.compile(optimizer = tf.keras.optimizers.Adam(lr_fsp_fnt), \n",
        "                  loss      = 'categorical_crossentropy', \n",
        "                  metrics   = ['accuracy'])\n",
        "\n",
        "\n",
        "model_fsp.summary()\n",
        "\n",
        "history_fsp_fnt = model_fsp.fit(datain_tr_fsp, \n",
        "                                dataou_tr_fsp, \n",
        "                                epochs           = epoch_fsp_fnt, \n",
        "                                batch_size       = batch_fsp_fnt, \n",
        "                                verbose          = 1, \n",
        "                                shuffle          = True)"
      ],
      "metadata": {
        "id": "K55ixp3vy8Rg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train the prx model using transfer learning and fine-tuning\n",
        "\n",
        "# Transfer learning\n",
        "\n",
        "model_base_prx.trainable      = False\n",
        "layer_batchnorm_prx.trainable = False\n",
        "\n",
        "model_prx.compile(optimizer = tf.keras.optimizers.Adam(lr_prx_trf), \n",
        "                  loss      = fun_simclr_loss, \n",
        "                  metrics   = 'mean_squared_error')\n",
        "\n",
        "model_prx.summary()\n",
        "\n",
        "model_prx, _ = fun_train_simclr(model_prx, \n",
        "                                datain_tr_prx, \n",
        "                                fun_augment_01, \n",
        "                                fun_augment_02, \n",
        "                                epochs     = epoch_prx_trf, \n",
        "                                batch_size = batch_prx_trf, \n",
        "                                verbose    = 1, \n",
        "                                patience   = 1)\n",
        "\n",
        "\n",
        "# Fine-tuning\n",
        "\n",
        "model_base_prx.trainable      = True\n",
        "layer_batchnorm_prx.trainable = True\n",
        "\n",
        "model_prx.compile(optimizer = tf.keras.optimizers.Adam(lr_prx_fnt), \n",
        "                  loss      = fun_simclr_loss, \n",
        "                  metrics   = 'mean_squared_error')\n",
        "\n",
        "model_prx.summary()\n",
        "\n",
        "model_prx, _ = fun_train_simclr(model_prx, \n",
        "                                datain_tr_prx, \n",
        "                                fun_augment_01, \n",
        "                                fun_augment_02, \n",
        "                                epochs     = epoch_prx_fnt, \n",
        "                                batch_size = batch_prx_fnt, \n",
        "                                verbose    = 1, \n",
        "                                patience   = 1)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cNTAM_ChlbOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create and train dwm model using transfer-learning and fine-tuning\n",
        "\n",
        "layerou_dwm = tf.keras.layers.Dense(dataou_tr_dwm.shape[-1], \n",
        "                                    activation = 'softmax')\n",
        "\n",
        "model_base_prx.trainable      = False\n",
        "layer_batchnorm_prx.trainable = False\n",
        "\n",
        "model_dwm   = tf.keras.models.Sequential([model_base_prx, \n",
        "                                          layer_batchnorm_prx, \n",
        "                                          layerou_dwm])\n",
        "\n",
        "model_dwm.layers[2].set_weights(layerou_fsp_initial_parameters)\n",
        "\n",
        "# Transfer learning\n",
        "model_dwm.compile(optimizer = tf.keras.optimizers.Adam(lr_dwm_trf), \n",
        "                  loss      = 'categorical_crossentropy', \n",
        "                  metrics   = ['accuracy'])\n",
        "\n",
        "model_dwm.summary()\n",
        "\n",
        "history_dwm = model_dwm.fit(datain_tr_dwm, \n",
        "                            dataou_tr_dwm, \n",
        "                            epochs           = epoch_dwm_trf, \n",
        "                            batch_size       = batch_dwm_trf, \n",
        "                            verbose          = 1, \n",
        "                            shuffle          = True)\n",
        "\n",
        "#Fine-tuning\n",
        "model_base_prx.trainable      = True\n",
        "layer_batchnorm_prx.trainable = True\n",
        "\n",
        "# We can fine-tune after certain model_base_prx layer!\n",
        "# fine_tune_after = 430\n",
        "# for layer in model_base_prx.layers[:fine_tune_after]:\n",
        "#   layer.trainable = False\n",
        "\n",
        "model_dwm.compile(optimizer = tf.keras.optimizers.Adam(lr_dwm_fnt), \n",
        "                  loss      = 'categorical_crossentropy', \n",
        "                  metrics   = ['accuracy'])\n",
        "\n",
        "model_dwm.summary()\n",
        "\n",
        "history_dwm = model_dwm.fit(datain_tr_dwm, \n",
        "                            dataou_tr_dwm, \n",
        "                            epochs           = epoch_dwm_fnt, \n",
        "                            batch_size       = batch_dwm_fnt, \n",
        "                            verbose          = 1, \n",
        "                            shuffle          = True)\n"
      ],
      "metadata": {
        "id": "za52rJgzm71v",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compute model_fsp and model_dwm testing accuracies\n",
        "_, acc_te_fsp = model_fsp.evaluate(datain_te, \n",
        "                                   dataou_te, \n",
        "                                   batch_size = 128)\n",
        "\n",
        "_, acc_te_dwm = model_dwm.evaluate(datain_te, \n",
        "                                   dataou_te, \n",
        "                                   batch_size = 128)\n",
        "\n",
        "print('Accuracy of fsp: {:05.2f}%'.format(acc_te_fsp*100))\n",
        "print('Accuracy of dwm: {:05.2f}%'.format(acc_te_dwm*100))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5p7gDls-y8Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clean up memory\n",
        "%reset"
      ],
      "metadata": {
        "cellView": "form",
        "id": "moW3GaaXy8Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 10: SimCLR, An Unsupervised Contrastive Pretext, Experiment**\n",
        "\n",
        "\n",
        "In this lecture, you learned about:\n",
        "\n",
        "1. A labeling problem with SimCLR using a pretrained encoder.\n",
        "\n",
        "\n",
        "> ***Congratulations on completing this course!***\n",
        "\n",
        "> * I hope this helps you learn SSL and how to apply it for labeling tasks.\n",
        "* You can use the same idea in data domains other than the image domain, such as temporal records and natural language processing.\n",
        "* $\\color{red}{\\text{Please rate this course and write a review.}}$\n",
        "\n",
        "Stay safe and sound!\n",
        "\n",
        "Mohammad H. Rafiei, Ph.D.\n",
        "\n",
        "* https://ep.jhu.edu/faculty/mohammad-rafiei/\n",
        "* https://scholar.google.com/citations?user=74pUQ3sAAAAJ&hl=en\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n"
      ],
      "metadata": {
        "id": "bBiYNuP3LRf-"
      }
    }
  ]
}